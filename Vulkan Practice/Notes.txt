I'm trying to learn Vulkan through this tutorial (APA reference):

Overvoorde, A. (2023). Vulkan Tutorial. https://vulkan-tutorial.com/.

If any kind of fatal error occurs during execution then we'll throw a std::runtime_error exception with a descriptive message,
which will propagate back to the main function and be printed to the command prompt. To handle a variety of standard exception
types as well, we catch the more general std::exception.

Just like each chunk of memory allocated with malloc requires a call to free, every Vulkan object that we create needs to be
explicitly destroyed when we no longer need it. In C++ it is possible to perform automatic resource management using RAII or
smart pointers provided in the <memory> header.

Vulkan objects are either created directly with functions like vkCreateXXX, or allocated through another object with functions
like vkAllocateXXX. After making sure that an object is no longer used anywhere, you need to destroy it with the counterparts
vkDestroyXXX and vkFreeXXX.

The very first thing you need to do is initialize the Vulkan library by creating an instance. The instance is the connection
between your application and the Vulkan library and creating it involves specifying some details about your application to
the driver.

If you look at the vkCreateInstance documentation then you'll see that one of the possible error codes is 
VK_ERROR_EXTENSION_NOT_PRESENT.

To retrieve a list of supported extensions before creating an instance, there's the vkEnumerateInstanceExtensionProperties
function. It takes a pointer to a variable that stores the number of extensions and an array of VkExtensionProperties to
store details of the extensions. It also takes an optional first parameter that allows us to filter extensions by a specific
validation layer.

Validation layers are optional components that hook into Vulkan function calls to apply additional operations.

You can simply enable validation layers for debug builds and completely disable them for release builds, which gives you the
best of both worlds.

To set up a callback in the program to handle messages and the associated details, we have to set up a debug messenger with
a callback using the VK_EXT_debug_utils extension.

debugCallback function's first parameter specifies the severity of the message:

- VK_DEBUG_UTILS_MESSAGE_SEVERITY_VERBOSE_BIT_EXT: Diagnostic message

- VK_DEBUG_UTILS_MESSAGE_SEVERITY_INFO_BIT_EXT: Informational message like the creation of a resource

- VK_DEBUG_UTILS_MESSAGE_SEVERITY_WARNING_BIT_EXT: Message about behavior that is not necessarily an error, but very likely a
bug in your application

- VK_DEBUG_UTILS_MESSAGE_SEVERITY_ERROR_BIT_EXT: Message about behavior that is invalid and may cause crashes

The messageType parameter of debugCallback function can have the following values:

- VK_DEBUG_UTILS_MESSAGE_TYPE_GENERAL_BIT_EXT: Some event has happened that is unrelated to the specification or performance

- VK_DEBUG_UTILS_MESSAGE_TYPE_VALIDATION_BIT_EXT: Something has happened that violates the specification or indicates a
possible mistake

- VK_DEBUG_UTILS_MESSAGE_TYPE_PERFORMANCE_BIT_EXT: Potential non-optimal use of Vulkan

The pCallbackData parameter refers to a VkDebugUtilsMessengerCallbackDataEXT struct containing the details of the message
itself, with the most important members being:

pMessage: The debug message as a null-terminated string
pObjects: Array of Vulkan object handles related to the message
objectCount: Number of objects in array

The pUserData parameter contains a pointer that was specified during the setup of the callback and allows you to pass your
own data to it.

Vulkan does not have the concept of a "default framebuffer", hence it requires an infrastructure that will own the buffers
we will render to before we visualize them on the screen. This infrastructure is known as the swap chain and must be created
explicitly in Vulkan. The swap chain is essentially a queue of images that are waiting to be presented to the screen. Our
application will acquire such an image to draw to it, and then return it to the queue.

Not all graphics cards are capable of presenting images directly to a screen for various reasons, for example because they
are designed for servers and don't have any display outputs. Secondly, since image presentation is heavily tied into the
window system and the surfaces associated with windows, it is not actually part of the Vulkan core. You have to enable the
VK_KHR_swapchain device extension after querying for its support.

Note that the Vulkan header file provides a nice macro VK_KHR_SWAPCHAIN_EXTENSION_NAME that is defined as VK_KHR_swapchain.
The advantage of using this macro is that the compiler will catch misspellings.

Just checking if a swap chain is available is not sufficient, because it may not actually be compatible with our window
surface. Creating a swap chain also involves a lot more settings than instance and device creation, so we need to query for some more details before we're able to proceed.

There are basically three kinds of properties we need to check:

1. Basic surface capabilities (min/max number of images in swap chain, min/max width and height of images)
2. Surface formats (pixel format, color space)
3. Available presentation modes

Each VkSurfaceFormatKHR entry contains a format and a colorSpace member. The format member specifies the color channels and
types. For example, VK_FORMAT_B8G8R8A8_SRGB means that we store the B, G, R and alpha channels in that order with an 8 bit
unsigned integer for a total of 32 bits per pixel. The colorSpace member indicates if the SRGB color space is supported or
not using the VK_COLOR_SPACE_SRGB_NONLINEAR_KHR flag. Note that this flag used to be called VK_COLORSPACE_SRGB_NONLINEAR_KHR
in old versions of the specification.

GLFW uses two units when measuring sizes: pixels and screen coordinates. For example, the resolution WIDTH, HEIGHT that we
specified earlier when creating the window is measured in screen coordinates. But Vulkan works with pixels, so the swap chain
extent must be specified in pixels as well. Unfortunately, if you are using a high DPI display (like Apple's Retina display),
screen coordinates don't correspond to pixels. Instead, due to the higher pixel density, the resolution of the window in pixel
will be larger than the resolution in screen coordinates. So if Vulkan doesn't fix the swap extent for us, we can't just use
the original WIDTH, HEIGHT. Instead, we must use glfwGetFramebufferSize to query the resolution of the window in pixel
before matching it against the minimum and maximum image extent.

To use any VkImage, including those in the swap chain, in the render pipeline we have to create a VkImageView object. An
image view is quite literally a view into an image. It describes how to access the image and which part of the image to
access, for example if it should be treated as a 2D texture depth texture without any mipmapping levels.

The input assembler collects the raw vertex data from the buffers you specify and may also use an index buffer to repeat
certain elements without having to duplicate the vertex data itself.

The vertex shader is run for every vertex and generally applies transformations to turn vertex positions from model space
to screen space. It also passes per-vertex data down the pipeline.

The tessellation shaders allow you to subdivide geometry based on certain rules to increase the mesh quality. This is often
used to make surfaces like brick walls and staircases look less flat when they are nearby.

The geometry shader is run on every primitive (triangle, line, point) and can discard it or output more primitives than came
in. This is similar to the tessellation shader, but much more flexible. However, it is not used much in today's applications
because the performance is not that good on most graphics cards except for Intel's integrated GPUs.

The rasterization stage discretizes the primitives into fragments. These are the pixel elements that they fill on the
framebuffer. Any fragments that fall outside the screen are discarded and the attributes outputted by the vertex shader
are interpolated across the fragments. Usually the fragments that are behind other primitive fragments are also discarded
here because of depth testing.

The fragment shader is invoked for every fragment that survives and determines which framebuffer(s) the fragments are written
to and with which color and depth values. It can do this using the interpolated data from the vertex shader, which can include
things like texture coordinates and normals for lighting.

The color blending stage applies operations to mix different fragments that map to the same pixel in the framebuffer.
Fragments can simply overwrite each other, add up or be mixed based upon transparency.

A clip coordinate is a four dimensional vector from the vertex shader that is subsequently turned into a normalized device
coordinate by dividing the whole vector by its last component. These normalized device coordinates are homogeneous coordinates
that map the framebuffer to a [-1, 1] by [-1, 1] coordinate system.

There is one more (optional) member, pSpecializationInfo, which we won't be using here, but is worth discussing. It allows
you to specify values for shader constants. You can use a single shader module where its behavior can be configured at
pipeline creation by specifying different values for the constants used in it. This is more efficient than configuring the
shader using variables at render time, because the compiler can do optimizations like eliminating if statements that depend
on these values. If you don't have any constants like that, then you can set the member to nullptr, which our struct
initialization does automatically.

A viewport basically describes the region of the framebuffer that the output will be rendered to. This will almost always
be (0, 0) to (width, height).

While viewports define the transformation from the image to the framebuffer, scissor rectangles define in which regions
pixels will actually be stored. Any pixels outside the scissor rectangles will be discarded by the rasterizer. They function
like a filter rather than a transformation.

If you are using a depth and/or stencil buffer, then you also need to configure the depth and stencil tests using
VkPipelineDepthStencilStateCreateInfo.

A single render pass can consist of multiple subpasses. Subpasses are subsequent rendering operations that depend on the
contents of framebuffers in previous passes, for example a sequence of post-processing effects that are applied one after
another.

We have to create a command pool before we can create command buffers. Command pools manage the memory that is used to store
the buffers and command buffers are allocated from them.

A semaphore is used to add order between queue operations. Queue operations refer to the work we submit to a queue, either in
a command buffer or from within a function. Examples of queues are the graphics queue and the presentation queue. Semaphores
are used both to order work inside the same queue and between different queues.

A fence has a similar purpose, in that it is used to synchronize execution, but it is for ordering the execution on the CPU,
otherwise known as the host. Simply put, if the host needs to know when the GPU has finished something, we use a fence.

Similar to semaphores, fences are either in a signaled or unsignaled state. Whenever we submit work to execute, we can attach
a fence to that work. When the work is finished, the fence will be signaled. Then we can make the host wait for the fence to
be signaled, guaranteeing that the work has finished before the host continues.

In general, it is preferable to not block the host unless necessary. We want to feed the GPU and the host with useful work to
do. Waiting on fences to signal is not useful work. Thus we prefer semaphores, or other synchronization primitives not yet
covered, to synchronize our work.

Fences must be reset manually to put them back into the unsignaled state. This is because fences are used to control the
execution of the host, and so the host gets to decide when to reset the fence. Contrast this to semaphores which are used to
order work on the GPU without the host being involved.

A vertex binding describes at which rate to load data from memory throughout the vertices. It specifies the number of bytes
between data entries and whether to move to the next data entry after each vertex or after each instance.

The binding parameter tells Vulkan from which binding the per-vertex data comes. The location parameter references the
location directive of the input in the vertex shader. The input in the vertex shader with location 0 is the position,
which has two 32-bit float components.

Buffers in Vulkan are regions of memory used for storing arbitrary data that can be read by the graphics card. They can be
used to store vertex data, but they can also be used for many other purposes too. Unlike most Vulkan objects, buffers do
not automatically allocate memory for themselves.

The flags parameter for VkBufferCreateInfo structure is used to configure sparse buffer memory.

The VkPhysicalDeviceMemoryProperties structure has two arrays memoryTypes and memoryHeaps. Memory heaps are distinct memory
resources like dedicated VRAM and swap space in RAM for when VRAM runs out.

An index buffer is essentially an array of pointers into the vertex buffer. It allows you to reorder the vertex data, and
reuse existing data for multiple vertices.

It is possible to use either uint16_t or uint32_t for your index buffer depending on the number of entries in vertices. We
can stick to uint16_t for now because we're using less than 65535 unique vertices.

Just like the vertex data, the indices need to be uploaded into a VkBuffer for the GPU to be able to access them.

